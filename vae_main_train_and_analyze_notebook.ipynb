{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofekrosenfeld1/thesis/blob/main/vae_main_train_and_analyze_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KczMppEqDCgI"
      },
      "outputs": [],
      "source": [
        "#import relevant libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import json\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import ast\n",
        "from multiprocessing import Pool\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from __future__ import generator_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mount drive"
      ],
      "metadata": {
        "id": "pf2koxu2uqbR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr2QeSkbjzNs",
        "outputId": "3c33d8a3-bfeb-4534-910f-fc31902e1f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#connect to google drive in order to mount files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data PreProcessing for model\n"
      ],
      "metadata": {
        "id": "7F-exQGLJau5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_and_selfies_indexing_jsons(file_name: str, idx_to_token_file: str, token_to_idx_file: str):\n",
        "  \"\"\"load the pre_processed data from chembl df, and also the pre-made jsons for selfies token/idx conversion\"\"\"\n",
        "  #load the chembl pre-processed data into the notebook\n",
        "  chembl_df = pd.read_csv('/content/drive/My Drive/chembl_relevant_info_with_numeric_selfies.csv')\n",
        "  chembl_df = chembl_df.drop(columns=['Unnamed: 0','Unnamed: 0.1','ChEMBL ID'])\n",
        "  #load the covnersion idxs jsons (selfies - numeric representation and vice versia)\n",
        "  with open(f'/content/drive/My Drive/{idx_to_token_file}.json','r') as f:\n",
        "    idx_to_tokens = json.load(f)\n",
        "  with open(f'/content/drive/My Drive/{token_to_idx_file}.json','r') as f:\n",
        "    tokens_to_idx = json.load(f)\n",
        "  return chembl_df, idx_to_tokens, tokens_to_idx\n"
      ],
      "metadata": {
        "id": "JU7Uuh637ZPf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_data_distr_for_optimal_paramaters(main_df : pd.DataFrame, percentile: float):\n",
        "  \"\"\"when training the VAE, we pad the sequences into constant value. Hence, we would not want a long sequence to let the generation go loose - hrut both training and generation. Its better to limit to the top x percentage of molecules with seq_len\"\"\"\n",
        "  main_df['seq_len'] = main_df['numeric_selfies'].str.count(',') + 1\n",
        "  seq_len = main_df['seq_len'].quantile(percentile)\n",
        "  return seq_len"
      ],
      "metadata": {
        "id": "VBYS6bPZ8fW6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7RYjf9xqvZWY"
      },
      "outputs": [],
      "source": [
        "def fast_parse_and_pad(s, seq_len, pad_value = 0):\n",
        "    \"\"\"\n",
        "    Converts stringified numeric SELFIE to a fixed-length tensor.\n",
        "    Truncates if too long, pads with `pad_value` if too short.\n",
        "    \"\"\"\n",
        "    tokens = list(map(int, s.strip('[]').replace(' ', '').split(',')))\n",
        "    tokens = tokens[:seq_len]  # Truncate if longer than max_len\n",
        "\n",
        "    # Pad to max_len\n",
        "    if len(tokens) < seq_len:\n",
        "        tokens += [pad_value] * (seq_len - len(tokens))\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "def process_molecules_fixed_length(df, column, max_len, pad_value=0):\n",
        "    \"\"\"\n",
        "    Vectorized fixed-length processing of numeric SELFIE strings.\n",
        "    Returns a list of torch tensors, one per molecule.\n",
        "    \"\"\"\n",
        "    return df[column].apply(lambda s: fast_parse_and_pad(s, max_len, pad_value)).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YrZovT71qlL3"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_model(tensors, batch_size):\n",
        "  \"\"\"the model accept as an input list of tensors\"\"\"\n",
        "  #decide which part of the tensors you want to use (more = slower training but more accurate)\n",
        "  tensors_for_model = tensors\n",
        "  #process the tensors to be a model-ready data\n",
        "  #stack the tensors to be in one tensor of shape [N, seq_len]\n",
        "  x_tensor = torch.stack(tensors_for_model)\n",
        "  #use pytorch approach of creating a dataset object of the big tensor we created before\n",
        "  dataset = TensorDataset(x_tensor)\n",
        "  #load the data using the needed batch_size, while shuffling the data every epoch\n",
        "  dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training and saving paramaters to drive"
      ],
      "metadata": {
        "id": "e5lqoVzNJhz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "mMqJ3MVz1iLr"
      },
      "outputs": [],
      "source": [
        "#the main VAE class, inherits from nn.Module\n",
        "class SelfiesVAE(nn.Module):\n",
        "    #init gets the base properties of the class\n",
        "    #vocab_size = number of tokens in the vocabulary (of the selfies)\n",
        "    #pad_idx = the token used to represent the padding\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim, pad_idx,seq_len):\n",
        "        #super().__init__() is needed for inheritance of nn.Module properties\n",
        "        super().__init__()\n",
        "        #the tensor by itself does not contribute any information - or chemical inforamtion, hence an embedding is needed for each SELFIE\n",
        "        #better than one-hot due to lower dim. vector and more information can be passed through embeddings\n",
        "        #padding_idx helps the embedding to know which index was used for padding the tensor - to not count them in the backprop\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        #after the embedding layer, the data is encoded into rnn (gru)\n",
        "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        #then, the mu and logvar are generated - latent space\n",
        "        self.to_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        #then, the latent space is converted into the hidden linear layer\n",
        "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        #after going through to the hidden layer, it is potentially possible to add act. func. -can experiment w/ that\n",
        "        #finally, we decode through a gru layer\n",
        "        self.decoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        #then - we finally have an output through a linear layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "        #the fixed lenght of a sequence (molecule), defined in the EDA steps\n",
        "        self.seq_len = seq_len\n",
        "        #padding index = 0\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        #x.long() makes the input in the correct format (int) into the embedding, embedding layer accepts integers\n",
        "        x_embed = self.embedding(x.long())\n",
        "        #gets the embedded and encode it through gru\n",
        "        #the output of the gru is outputs, h - but we just want the h, not all the outputs. we are interested in the summery vector of all the timesteps in the gru\n",
        "        _, h = self.encoder_rnn(x_embed)\n",
        "        #take just the needed dimentions\n",
        "        h = h.squeeze(0)\n",
        "        #get the mu out of h\n",
        "        mu = self.to_mu(h)\n",
        "        #get the logvar out of h\n",
        "        logvar = self.to_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        #reparametrization trick - using z = mu + std*eps~N(0,1), this allows to backprop through mu,std - while eps~N(0,1) stays the same\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, seq_len):\n",
        "        #first, the z latent vector is decoded through linear layer that maps it into the shpe expected by the gru\n",
        "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
        "        #get the batch size through z(0) - z is [batch_size, latent_vec_size]\n",
        "        batch_size = z.size(0)\n",
        "        #we need an initial input token to insert the gru - using the pad_idx as initial token\n",
        "        #the shape of the initial input is [batch_size,1]\n",
        "        #torch.long to match the expected type of embedded, then sending it to device\n",
        "        #the \"dummy input token\" is common usage as an initializer of the gru\n",
        "        input_token = torch.full((batch_size, 1), self.pad_idx, dtype=torch.long, device=z.device)\n",
        "        outputs = []\n",
        "\n",
        "        #each time the gru predicts the next token, so we iterate on each token in the seq_len\n",
        "        for _ in range(seq_len):\n",
        "            #get the embdedding of the input token - to get embedding vector of the token\n",
        "            embed = self.embedding(input_token)\n",
        "            #get the prediction (out) of the next token from the gru - based on the previous token\n",
        "            out, hidden = self.decoder_rnn(embed, hidden)\n",
        "            #logits are with shape [batch_size, vocab_size] meaning it maps the probability of each token from the vocabulary based on the output from the gru\n",
        "            #now, instead of each token we have a vector w/ probability of each possible token. remember we have a batch each time so its [batch_size, vocab_size]\n",
        "            logits = self.output_layer(out.squeeze(1))\n",
        "            #save the logits of each timestep\n",
        "            outputs.append(logits)\n",
        "            input_token = logits.argmax(dim=1, keepdim=True)  # Greedy decoding\n",
        "        #the results is stacked list of the output logits for each timestep\n",
        "        return torch.stack(outputs, dim=1)  # [B, L, V]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      #forward is important func. - as it inherits from nn.Module to be activates once the model is called\n",
        "      #the output of the forward is the logits (distr. for each token in each timestep) after the decoding, plus the mu,std of the latent space\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat_logits = self.decode(z, x.size(1))\n",
        "        return x_hat_logits, mu, logvar\n",
        "\n",
        "\n",
        "# === Loss Function ===\n",
        "\n",
        "def vae_loss(x_hat_logits, x_true, mu, logvar, pad_idx):\n",
        "  #we use the cross_entropy as the reconstruction loss - makes the most sense\n",
        "  #here we do it token by token\n",
        "  #the -1 flattens it\n",
        "  #ignoring the pad index for loss calc ofc\n",
        "  #\n",
        "    recon_loss = F.cross_entropy(\n",
        "        x_hat_logits.view(-1, x_hat_logits.size(-1)),\n",
        "        x_true.view(-1),\n",
        "        ignore_index=pad_idx,\n",
        "        reduction='mean'\n",
        "    )\n",
        "    #also, we use the kl divergence as a regularization that the latent dist. behave like a gaussian dist. - keepin it as smooth as possible\n",
        "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_div, recon_loss, kl_div\n",
        "\n",
        "\n",
        "# === Training Setup ===\n",
        "\n",
        "def train_vae(model, dataloader, optimizer, device, pad_idx, num_epochs):\n",
        "    #sets the params into training setup\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_recon = 0\n",
        "        total_kl = 0\n",
        "        #loop is an approach to define dataloader just with tdqm method(for process bars)\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        #looping through each batch\n",
        "        for (x_batch,) in loop:\n",
        "            x_batch = x_batch.to(device)\n",
        "            #first resetting the gradients in the optimizer - independent each batch\n",
        "            optimizer.zero_grad()\n",
        "            #calc the x_hat, mu,var with model(batch) -> applying forward method\n",
        "            x_hat_logits, mu, logvar = model(x_batch)\n",
        "            #calc loss of the output\n",
        "            loss, recon, kl = vae_loss(x_hat_logits, x_batch, mu, logvar, pad_idx)\n",
        "            #do backprop based on the loss\n",
        "            loss.backward()\n",
        "            #update the model weights properly to the backprop\n",
        "            optimizer.step()\n",
        "            #collecting info for each loop to estimate how the model did over epochs\n",
        "            #loss is a tensor, hence by .item() you get the number only\n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon.item()\n",
        "            total_kl += kl.item()\n",
        "            loop.set_postfix(loss=loss.item(), recon=recon.item(), kl=kl.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Loss: {total_loss:.2f}, Recon: {total_recon:.2f}, KL: {total_kl:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL5Btrpxbcco"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "lwW8eOBgW9X8"
      },
      "outputs": [],
      "source": [
        "def train_the_model(idx_to_token: dict,\n",
        "                    seq_len: int, #len of generated molecule\n",
        "                    embedding_dim: int, #embedding layer dimention\n",
        "                    hidden_dim: int, #hidden layer dimention\n",
        "                    latent_dim: int, #latent layer dim\n",
        "                    num_epochs: int,\n",
        "                    learning_rate: float,\n",
        "                    batch_size: int,\n",
        "                    tensors: list,\n",
        "                    dataloader: DataLoader, #setup of data to the model\n",
        "                    pad_idx = 0 #padding layer, to indicate the model to ignore it in the loss\n",
        "                    ):\n",
        "    #vocab size is the size of your unique tokens in your index, including padding idx\n",
        "    vocab_size = len(idx_to_token)\n",
        "    #set up the model with the dimenstions mentioned before\n",
        "    model = SelfiesVAE(vocab_size, embedding_dim, hidden_dim, latent_dim, pad_idx,seq_len)\n",
        "    #set up the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #send model to device\n",
        "    model = SelfiesVAE(vocab_size, embedding_dim, hidden_dim, latent_dim, pad_idx,seq_len).to(device)\n",
        "    #set up the ADAM optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # Train the model\n",
        "    train_vae(model, dataloader, optimizer, device, pad_idx, num_epochs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "nHeOXp9l8N75"
      },
      "outputs": [],
      "source": [
        "def save_model(model):\n",
        "  #saving the model paramaters in drive to re-use them later on for further experiment\n",
        "  # save only the model parameters (recommended)\n",
        "  torch.save(model.state_dict(), 'vae_selfies_all_data_exp.pt')\n",
        "  #download to the local machine (optional)\n",
        "  from google.colab import files\n",
        "  files.download('vae_selfies_all_data_exp.pt')\n",
        "  #save to google drive\n",
        "  torch.save(model.state_dict(), '/content/drive/MyDrive/vae_selfies_all_data_exp.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the model and analyzing"
      ],
      "metadata": {
        "id": "EyhlT7hSJrdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZbLQrqtzK-fK"
      },
      "outputs": [],
      "source": [
        "def load_trained_model(vocab_size: int, embedding_dim: int, hidden_dim: int,\n",
        "                       latent_dim: int, pad_idx: int, seq_len: int, model_name: str):\n",
        "  \"\"\"Once finished training, we can load the model annd analyze it\"\"\"\n",
        "  #set up the model archetecture\n",
        "  model = SelfiesVAE(vocab_size, embedding_dim, hidden_dim, latent_dim, pad_idx,seq_len)\n",
        "  #set up the device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  #send model to device\n",
        "  model = SelfiesVAE(vocab_size, embedding_dim, hidden_dim, latent_dim, pad_idx,seq_len).to(device)\n",
        "  #load the model paramaters\n",
        "  model.load_state_dict(torch.load(f'/content/drive/MyDrive/{model_name}'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vDOeDmNj2tR8"
      },
      "outputs": [],
      "source": [
        "def generate_molecule_from_trained_model(model, seed_idx, seq_len, idx_to_token, device, tensor_idx):\n",
        "  \"\"\"once model is already trained, we can do analysis / research of its operations / latent space\"\"\"\n",
        "  #choose the index of the tensor of the seed molecule\n",
        "  mol = tensors[tensor_idx]\n",
        "  print(mol)\n",
        "  #send to device\n",
        "  x = mol.unsqueeze(0).to(device)\n",
        "  #use eval mode - to not accidently activate all the behaviors of .train() mode\n",
        "  # make sure model is in eval mode - no_grad saves memory and time, ensure inference mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      #encode the mu, logvar that represents your molecule\n",
        "      mu, logvar = model.encode(x)\n",
        "      z = model.reparameterize(mu, logvar)  # shape: [1, latent_dim]\n",
        "      #decode your sampled molecule from the latent space\n",
        "      logits = model.decode(z, seq_len)\n",
        "      token_ids = logits.argmax(dim=2)[0]  # [seq_len]\n",
        "      #get the original molecule's representation to compare to the generated\n",
        "      generated_tokens = [idx_to_token[str(i.item())] for i in token_ids]\n",
        "      #get the chemical represnetation of the generated molecule\n",
        "      original_mol = [idx_to_token[str(i.item())] for i in x[0]]\n",
        "      return generated_tokens, original_mol\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main running function"
      ],
      "metadata": {
        "id": "U0GmucNDJz1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__== \"__main__\":\n",
        "  #load the data\n",
        "  main_df, idx_to_tokens, tokens_to_idx = load_data_and_selfies_indexing_jsons(file_name = 'chembl_relevant_info_with_numeric_selfies.csv',\n",
        "                                                                               idx_to_token_file = 'idx_to_token',\n",
        "                                                                               token_to_idx_file = 'token_to_indx')\n",
        "  #setting up the model's paramaters\n",
        "  vocab_size = len(idx_to_tokens)\n",
        "  #seq_len is the maximum len of a tensor for training and generation of the vae\n",
        "  seq_len = int(analyze_data_distr_for_optimal_paramaters(main_df = main_df, percentile = 0.95))\n",
        "  embedding_dim = 256\n",
        "  hidden_dim = 256\n",
        "  latent_dim = 64\n",
        "  #padding value for shorter molecules (then the seq_len)\n",
        "  pad_idx = 0\n",
        "  num_epochs = 20\n",
        "  batch_size = 64\n",
        "  learning_rate = 1e-3\n",
        "  #get the tensors ready for a model\n",
        "  tensors = process_molecules_fixed_length(df = main_df, column ='numeric_selfies', max_len = seq_len, pad_value = pad_idx)\n",
        "  #prepare the data to be model - ready\n",
        "  dataloader = prepare_data_for_model(tensors = tensors, batch_size = batch_size)\n",
        "  #train the model\n",
        "  model = train_the_model(idx_to_token = idx_to_tokens,\n",
        "                          seq_len = seq_len,\n",
        "                          embedding_dim = embedding_dim,\n",
        "                          hidden_dim = hidden_dim,\n",
        "                          latent_dim = latent_dim,\n",
        "                          num_epochs = num_epochs,\n",
        "                          learning_rate = learning_rate,\n",
        "                          batch_size = batch_size,\n",
        "                          tensors = tensors,\n",
        "                          dataloader = dataloader,\n",
        "                          pad_idx = pad_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "collapsed": true,
        "id": "DyoqhNJOJ4tg",
        "outputId": "14c5799a-d3c3-46f1-ef72-319d54cbbfc3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "train_vae() missing 4 required positional arguments: 'learning_rate', 'batch_size', 'tensors', and 'dataloader'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-1582225303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   model = train_the_model(idx_to_token = idx_to_tokens,\n\u001b[0m\u001b[1;32m     24\u001b[0m               \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m               \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-616825487.py\u001b[0m in \u001b[0;36mtrain_the_model\u001b[0;34m(idx_to_token, seq_len, embedding_dim, hidden_dim, latent_dim, num_epochs, learning_rate, batch_size, tensors, dataloader, pad_idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train_vae() missing 4 required positional arguments: 'learning_rate', 'batch_size', 'tensors', and 'dataloader'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV381-nKQG7d",
        "outputId": "e9d519f0-dd36-4a10-f7da-427a9f16f3b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  thesis\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNeMAhqPKsAZwPMDtEM7R5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}